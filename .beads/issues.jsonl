{"id":"pydantic-ai-gepa-51k","title":"Agent Skills: spec alignment + skills optimization","description":"Align ../indexed skill packs with Agent Skills spec (agentskills.io), then add first-class skill optimization to pydantic-ai-gepa (descriptions, bodies, examples) with Turbopuffer-backed search over updated content.","design":"Background:\n- ../indexed already has a skills system (SKILL.md + frontmatter extensions like boundary/toolsets/playbooks/...)\n- pydantic-ai-gepa optimizes prompt/tool/signature components via GEPA; we want to extend it to skill content and keep search working as skills evolve.\n\nGoals:\n- Make ../indexed skill packs compatible with Agent Skills spec fields/constraints (name/description + optional license/compatibility/metadata/allowed-tools), while preserving Indexed-specific extensions as needed.\n- Add a filesystem-first SkillsFS + overlay model for skills in pydantic-ai-gepa; expose safe, structured optimization components per skill (e.g. description/body/examples).\n- Support Turbopuffer-backed hybrid search that reflects candidate overlays (updated skill content) without needing external merge infra.\n- Ensure component selection scales to hundreds of skills via hierarchical selection (skill-level grouping) and trace-aware prioritization; optionally allow LLM-assisted routing under tight budgets.\n\nNon-goals (initially):\n- Executing arbitrary skill scripts in optimization runs (can be a later opt-in).\n- Large-scale repo-wide content rewrite beyond what is needed for spec alignment.\n\nKey open decisions:\n- Indexed naming strategy: how to satisfy Agent Skills name constraints (must match parent dir, lowercase alnum+hyphen) given current nested skill paths (e.g. index/tasks, spaces/members/list).\n- Where Indexed-specific fields live after alignment (keep as extension fields vs encode into metadata).\n- Default selection strategy for skill components at scale (see subtasks).","acceptance_criteria":"- [ ] All child issues are completed\n- [ ] Integration tests pass\n- [ ] Documentation is updated\n- [ ] Code review completed\n","status":"open","priority":1,"issue_type":"epic","created_at":"2025-12-19T13:44:05.179507-08:00","updated_at":"2025-12-19T13:44:26.250863-08:00","labels":["epic","skills"]}
{"id":"pydantic-ai-gepa-51k.1","title":"Indexed: align skills with Agent Skills spec","description":"Update ../indexed SKILL.md format + parsing/indexing to match agentskills.io specification, while preserving Indexed-specific extensions (boundary/toolsets/etc.) in a forward-compatible way.","status":"open","priority":1,"issue_type":"epic","created_at":"2025-12-19T13:44:32.742989-08:00","updated_at":"2025-12-19T13:44:32.742989-08:00","labels":["epic","indexed","skills"],"dependencies":[{"issue_id":"pydantic-ai-gepa-51k.1","depends_on_id":"pydantic-ai-gepa-51k","type":"parent-child","created_at":"2025-12-19T13:44:32.743398-08:00","created_by":"daemon","metadata":"{}"}]}
{"id":"pydantic-ai-gepa-51k.1.1","title":"Indexed: decide spec-aligned skill naming strategy","description":"Pick a migration strategy to satisfy Agent Skills name constraints (matches directory name; lowercase alnum+hyphen) given Indexed’s nested skill paths (e.g. index/tasks, spaces/members/list). Output a short decision doc with implications for search keys, display names, and backwards compatibility.","acceptance_criteria":"- Decision doc covers: keep nested dirs vs flatten; collision handling; how skill_path/id is represented; whether name becomes leaf dir vs unique hyphenated path; how to preserve existing toolset gating keyed by skill_path.","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-19T13:44:58.240682-08:00","updated_at":"2025-12-19T13:45:03.468499-08:00","labels":["design","indexed","skills"],"dependencies":[{"issue_id":"pydantic-ai-gepa-51k.1.1","depends_on_id":"pydantic-ai-gepa-51k.1","type":"parent-child","created_at":"2025-12-19T13:44:58.241077-08:00","created_by":"daemon","metadata":"{}"}]}
{"id":"pydantic-ai-gepa-51k.1.2","title":"Indexed: extend SkillFrontmatter parsing to Agent Skills spec","description":"Update ../indexed SkillFrontmatter/registry to support Agent Skills spec fields (name, description, license, compatibility, metadata, allowed-tools) while preserving Indexed extensions (boundary/toolsets/playbooks/schemas/scripts/references/examples). Add validation where feasible (name matches directory, allowed chars, length constraints) and decide how strict to be.","acceptance_criteria":"- Registry can parse both old and new skills during migration window (if needed)\n- Validation errors are actionable and point to skill_path\n- Indexed-specific fields still work (boundary/toolsets/etc.)","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-19T13:45:09.804717-08:00","updated_at":"2025-12-19T13:45:09.804717-08:00","labels":["indexed","skills"],"dependencies":[{"issue_id":"pydantic-ai-gepa-51k.1.2","depends_on_id":"pydantic-ai-gepa-51k.1","type":"parent-child","created_at":"2025-12-19T13:45:09.805155-08:00","created_by":"daemon","metadata":"{}"},{"issue_id":"pydantic-ai-gepa-51k.1.2","depends_on_id":"pydantic-ai-gepa-51k.1.1","type":"blocks","created_at":"2025-12-19T13:45:46.421725-08:00","created_by":"daemon","metadata":"{}"}]}
{"id":"pydantic-ai-gepa-51k.1.3","title":"Indexed: migrate api/skills SKILL.md frontmatter to spec-compliant names","description":"Update existing ../indexed/api/skills/**/SKILL.md frontmatter so required Agent Skills fields are spec-compliant (name matches directory, length/charset) and optional fields are placed appropriately. Ensure search/skill tools continue to use skill_path as the canonical ID.","acceptance_criteria":"- All SKILL.md validate under chosen policy\n- No runtime regressions in list_skills/load_skill/search_skills\n- If name collisions exist, documented and resolved","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-19T13:45:15.22373-08:00","updated_at":"2025-12-19T13:45:15.22373-08:00","labels":["indexed","skills"],"dependencies":[{"issue_id":"pydantic-ai-gepa-51k.1.3","depends_on_id":"pydantic-ai-gepa-51k.1","type":"parent-child","created_at":"2025-12-19T13:45:15.224102-08:00","created_by":"daemon","metadata":"{}"},{"issue_id":"pydantic-ai-gepa-51k.1.3","depends_on_id":"pydantic-ai-gepa-51k.1.2","type":"blocks","created_at":"2025-12-19T13:45:46.517657-08:00","created_by":"daemon","metadata":"{}"}]}
{"id":"pydantic-ai-gepa-51k.1.4","title":"Indexed: update skills turbopuffer indexing/search to include spec fields","description":"Update ../indexed/api/scripts/skills_index.py and ../indexed/api/src/api/services/search/skills.py to align stored attributes with Agent Skills spec fields. Ensure doc_type classification still works and that updated name/description are indexed and boosted.","acceptance_criteria":"- Reindex produces rows with correct name/description and optional spec fields if present\n- search_skills ranking still boosts name/description\n- Any schema migration is backwards compatible or includes reindex instructions","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-19T13:45:20.940973-08:00","updated_at":"2025-12-19T13:45:20.940973-08:00","labels":["indexed","search","skills"],"dependencies":[{"issue_id":"pydantic-ai-gepa-51k.1.4","depends_on_id":"pydantic-ai-gepa-51k.1","type":"parent-child","created_at":"2025-12-19T13:45:20.94139-08:00","created_by":"daemon","metadata":"{}"},{"issue_id":"pydantic-ai-gepa-51k.1.4","depends_on_id":"pydantic-ai-gepa-51k.1.2","type":"blocks","created_at":"2025-12-19T13:45:46.613959-08:00","created_by":"daemon","metadata":"{}"}]}
{"id":"pydantic-ai-gepa-51k.1.5","title":"Indexed: add skills validation tooling (skills-ref + CI checks)","description":"Add a validation step for skills packs (optionally using agentskills/skills-ref) and/or a local validator, so spec drift is caught early. Document how to run it and how strict it is.","acceptance_criteria":"- CI (or a documented local command) validates SKILL.md frontmatter constraints\n- Clear error messages for failing skills\n- Decision on whether to vendor, pin, or wrap skills-ref","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-19T13:45:26.516476-08:00","updated_at":"2025-12-19T13:45:26.516476-08:00","labels":["ci","indexed","skills"],"dependencies":[{"issue_id":"pydantic-ai-gepa-51k.1.5","depends_on_id":"pydantic-ai-gepa-51k.1","type":"parent-child","created_at":"2025-12-19T13:45:26.516865-08:00","created_by":"daemon","metadata":"{}"},{"issue_id":"pydantic-ai-gepa-51k.1.5","depends_on_id":"pydantic-ai-gepa-51k.1.2","type":"blocks","created_at":"2025-12-19T13:45:46.726888-08:00","created_by":"daemon","metadata":"{}"},{"issue_id":"pydantic-ai-gepa-51k.1.5","depends_on_id":"pydantic-ai-gepa-51k.1.3","type":"blocks","created_at":"2025-12-19T13:45:46.841344-08:00","created_by":"daemon","metadata":"{}"}]}
{"id":"pydantic-ai-gepa-51k.2","title":"pydantic-ai-gepa: optimize skill content (descriptions/bodies/examples)","description":"Add first-class skill components to GEPA (safe structured fields) and integrate a skills toolset so student agents can search/load skill content during evaluation.","notes":"All child tasks completed (skills FS, components incl examples/, toolset, search backends, selector scaling, docs/example).","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-19T13:44:37.56342-08:00","updated_at":"2025-12-19T14:47:04.904673-08:00","closed_at":"2025-12-19T14:47:04.904675-08:00","labels":["epic","gepa","skills"],"dependencies":[{"issue_id":"pydantic-ai-gepa-51k.2","depends_on_id":"pydantic-ai-gepa-51k","type":"parent-child","created_at":"2025-12-19T13:44:37.563775-08:00","created_by":"daemon","metadata":"{}"}]}
{"id":"pydantic-ai-gepa-51k.2.1","title":"GEPA Skills: define component model and selection strategy at scale","description":"Write a concrete design for: (1) SkillsFS representation and overlay semantics, (2) which skill subcomponents are optimizable (description/body/examples/files), (3) how to keep the component set tractable with 100s of skills, and (4) whether/how to allow LLM-assisted routing of which skill/component to edit.","design":"Decisions (2025-12-19)\n\n1) Canonical in-memory representation\n- Use a filesystem-first model (SkillsFS) with Node = Directory|File.\n- A “skill” is any directory containing a SKILL.md file.\n- Overlay is a union FS (OverlayFS(base, overlay)), so candidates can add/replace files without mutating base.\n\n2) Parsing and compatibility\n- Parse SKILL.md frontmatter per Agent Skills spec fields:\n  - required: name, description\n  - optional: license, compatibility, metadata (map), allowed-tools (string)\n- Also support Indexed extensions (forward compatible): boundary, toolsets, playbooks, schemas, scripts, references, examples.\n  - Treat these as optional extension fields during parsing; do not require them.\n\n3) Optimizable components (safe structured edits)\n- Do not let reflection rewrite raw YAML.\n- Expose structured skill components (per skill_path):\n  - skill:{skill_path}:frontmatter:description\n  - skill:{skill_path}:body\n  - (phase 2) skill:{skill_path}:examples (single aggregated component) OR bounded file components skill:{skill_path}:file:examples/\u003cname\u003e.\n- Candidate application re-renders SKILL.md (frontmatter rebuilt from structured fields + preserved extension fields) + updates FS overlay.\n\n4) Search with Turbopuffer (Indexed-aligned)\n- Support base namespace + per-candidate overlay namespace.\n- Deterministic row IDs: {skill_path}:{file_path}:{chunk_index}.\n- Query overlay first and base second; merge by row id so overlay wins.\n- Overlay indexing writes only changed skill files for the candidate; deletes stale chunks for those skill_path values.\n\n5) Component selection at scale\n- For now, use the new module_selector=\"llm\" mode for routing when component sets are large.\n  - Input to selector: allowed component names + minibatch eval_results (scores/errors + trajectory reflective records).\n  - Output: \u003c=N component names, validated against allowed list.\n- Default max components per iteration for llm selector: 1 (configurable via llm_component_selector_max_components).\n- Follow-up (optional): add a hierarchical skills selector that chooses a skill_path then subcomponents; not required for MVP since llm selector can route.\n\n6) API knobs\n- optimize_agent(..., module_selector=\"llm\", llm_component_selector_max_components=N) controls LLM-based routing.\n- Skills integration will add additional knobs (skills_root / SkillsFS input, turbopuffer namespaces) when implementing pydantic-ai-gepa-51k.2.2+.\\n","acceptance_criteria":"- Defines component key scheme and boundaries (what is editable)\n- Defines selection strategies and defaults (hierarchical skill-level round robin, trace-aware prioritization)\n- Specifies limits (max skills considered per iteration, max files per skill, etc.)\n- Decides if LLM routing is used and under what budgets/guards","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-19T13:46:02.214594-08:00","updated_at":"2025-12-19T14:07:15.74322-08:00","closed_at":"2025-12-19T14:07:15.743223-08:00","labels":["design","gepa","skills"],"dependencies":[{"issue_id":"pydantic-ai-gepa-51k.2.1","depends_on_id":"pydantic-ai-gepa-51k.2","type":"parent-child","created_at":"2025-12-19T13:46:02.214957-08:00","created_by":"daemon","metadata":"{}"}]}
{"id":"pydantic-ai-gepa-51k.2.2","title":"Implement SkillsFS + overlay + SKILL.md parsing","description":"Implement a filesystem-first SkillsFS (Directory/File nodes), an OverlayFS (base+overlay union), and a parser/renderer for SKILL.md frontmatter+body aligned with Agent Skills spec (plus extensions).","acceptance_criteria":"- Load skills from a directory tree (find **/SKILL.md)\n- Parse frontmatter fields per spec (name/description + optional)\n- Overlay can add/replace files without mutating base\n- Stable hashing for SKILL.md body and referenced files","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-19T13:46:09.704604-08:00","updated_at":"2025-12-19T14:11:09.978286-08:00","closed_at":"2025-12-19T14:11:09.978289-08:00","labels":["gepa","skills"],"dependencies":[{"issue_id":"pydantic-ai-gepa-51k.2.2","depends_on_id":"pydantic-ai-gepa-51k.2","type":"parent-child","created_at":"2025-12-19T13:46:09.705053-08:00","created_by":"daemon","metadata":"{}"},{"issue_id":"pydantic-ai-gepa-51k.2.2","depends_on_id":"pydantic-ai-gepa-51k.2.1","type":"blocks","created_at":"2025-12-19T13:46:52.009173-08:00","created_by":"daemon","metadata":"{}"}]}
{"id":"pydantic-ai-gepa-51k.2.3","title":"Expose skill components to GEPA and apply candidates safely","description":"Define and implement GEPA components for skills (e.g. per-skill description, body, optional example bundles/files) and candidate application that re-renders SKILL.md safely (no raw YAML editing by reflection).","acceptance_criteria":"- Seed extraction returns skill components in candidate map\n- Candidate application updates the SkillsFS overlay deterministically\n- Component validation prevents malformed output (empty description, etc.)\n- Works with SignatureAgentAdapter evaluation runs","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-19T13:46:15.33619-08:00","updated_at":"2025-12-19T14:12:41.83918-08:00","closed_at":"2025-12-19T14:12:41.839182-08:00","labels":["gepa","skills"],"dependencies":[{"issue_id":"pydantic-ai-gepa-51k.2.3","depends_on_id":"pydantic-ai-gepa-51k.2","type":"parent-child","created_at":"2025-12-19T13:46:15.336576-08:00","created_by":"daemon","metadata":"{}"},{"issue_id":"pydantic-ai-gepa-51k.2.3","depends_on_id":"pydantic-ai-gepa-51k.2.2","type":"blocks","created_at":"2025-12-19T13:46:52.132276-08:00","created_by":"daemon","metadata":"{}"},{"issue_id":"pydantic-ai-gepa-51k.2.3","depends_on_id":"pydantic-ai-gepa-51k.2.1","type":"blocks","created_at":"2025-12-19T13:46:52.238974-08:00","created_by":"daemon","metadata":"{}"}]}
{"id":"pydantic-ai-gepa-51k.2.4","title":"Add skills toolset for student agents (list/search/load/load_file)","description":"Add a pydantic-ai FunctionToolset that lets the student agent discover and load skills from the current SkillsFS (including overlayed candidate edits). Include a local search fallback (BM25-ish) for development.","acceptance_criteria":"- Tools: list_skills, search_skills, load_skill, load_skill_file\n- Uses skill_path as canonical identifier\n- Search reflects overlay edits\n- Can be injected via SignatureAgent.run_signature and adapters","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-19T13:46:21.105274-08:00","updated_at":"2025-12-19T14:19:25.457094-08:00","closed_at":"2025-12-19T14:19:25.457097-08:00","labels":["gepa","skills"],"dependencies":[{"issue_id":"pydantic-ai-gepa-51k.2.4","depends_on_id":"pydantic-ai-gepa-51k.2","type":"parent-child","created_at":"2025-12-19T13:46:21.105643-08:00","created_by":"daemon","metadata":"{}"},{"issue_id":"pydantic-ai-gepa-51k.2.4","depends_on_id":"pydantic-ai-gepa-51k.2.2","type":"blocks","created_at":"2025-12-19T13:46:52.335673-08:00","created_by":"daemon","metadata":"{}"}]}
{"id":"pydantic-ai-gepa-51k.2.5","title":"Turbopuffer-backed skills search with per-candidate overlay namespaces","description":"Implement an optional Turbopuffer indexer and query path modeled after ../indexed: base namespace + per-candidate overlay namespace. Query overlay then base and merge results by row id so overlay wins. Support hybrid search when embeddings are available.","acceptance_criteria":"- Deterministic row ids (skill_path:file_path:chunk_index)\n- Overlay namespace contains only changed skill files for the candidate\n- Query merge rule: overlay rows override base rows for the same id\n- Concurrency-safe for parallel candidate evaluation (namespace per candidate)\n- Clear cleanup policy for ephemeral namespaces","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-19T13:46:27.548082-08:00","updated_at":"2025-12-19T14:32:42.922786-08:00","closed_at":"2025-12-19T14:32:42.922796-08:00","labels":["gepa","search","skills"],"dependencies":[{"issue_id":"pydantic-ai-gepa-51k.2.5","depends_on_id":"pydantic-ai-gepa-51k.2","type":"parent-child","created_at":"2025-12-19T13:46:27.548458-08:00","created_by":"daemon","metadata":"{}"},{"issue_id":"pydantic-ai-gepa-51k.2.5","depends_on_id":"pydantic-ai-gepa-51k.2.2","type":"blocks","created_at":"2025-12-19T13:46:52.433433-08:00","created_by":"daemon","metadata":"{}"},{"issue_id":"pydantic-ai-gepa-51k.2.5","depends_on_id":"pydantic-ai-gepa-51k.2.4","type":"blocks","created_at":"2025-12-19T13:46:52.555214-08:00","created_by":"daemon","metadata":"{}"}]}
{"id":"pydantic-ai-gepa-51k.2.6","title":"Skills component selection: hierarchical + trace-aware prioritization","description":"Implement scalable component selection for skills: pick a skill (or small set) per iteration, then pick subcomponents (description/body/examples) with configurable caps. Use trace signals (loaded skills, search results, feedback) to prioritize which skills to optimize.","acceptance_criteria":"- New selector strategy (e.g. skills_round_robin / skills_trace_aware)\n- Does not require enumerating hundreds of skill components in every reflection prompt\n- Has guardrails: max_skills_per_iter, max_skill_files_per_iter\n- Works with existing round_robin/all for non-skill components","notes":"Implemented scalable LLM component selection via max allowed-components prefiltering; surfaced llm_component_selector_max_allowed_components in optimize_agent; added tests.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-19T13:46:33.657834-08:00","updated_at":"2025-12-19T14:47:04.708527-08:00","closed_at":"2025-12-19T14:47:04.708531-08:00","labels":["gepa","skills"],"dependencies":[{"issue_id":"pydantic-ai-gepa-51k.2.6","depends_on_id":"pydantic-ai-gepa-51k.2","type":"parent-child","created_at":"2025-12-19T13:46:33.658165-08:00","created_by":"daemon","metadata":"{}"},{"issue_id":"pydantic-ai-gepa-51k.2.6","depends_on_id":"pydantic-ai-gepa-51k.2.1","type":"blocks","created_at":"2025-12-19T13:46:52.664863-08:00","created_by":"daemon","metadata":"{}"},{"issue_id":"pydantic-ai-gepa-51k.2.6","depends_on_id":"pydantic-ai-gepa-51k.2.3","type":"blocks","created_at":"2025-12-19T13:46:52.776917-08:00","created_by":"daemon","metadata":"{}"}]}
{"id":"pydantic-ai-gepa-51k.2.7","title":"Docs + example: optimize skills pack with GEPA","description":"Add a small example skills pack and a runnable example showing GEPA optimizing skill description/body/examples and using skills search/load during evaluation.","acceptance_criteria":"- Example runs with uv run\n- Documents required env vars for Turbopuffer (if used) and local fallback","notes":"Added example skills pack + examples/optimize_skills.py; documented local fallback vs TurboPuffer and TURBOPUFFER_API_KEY.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-19T13:46:38.626461-08:00","updated_at":"2025-12-19T14:47:04.810967-08:00","closed_at":"2025-12-19T14:47:04.810969-08:00","labels":["docs","skills"],"dependencies":[{"issue_id":"pydantic-ai-gepa-51k.2.7","depends_on_id":"pydantic-ai-gepa-51k.2","type":"parent-child","created_at":"2025-12-19T13:46:38.626809-08:00","created_by":"daemon","metadata":"{}"},{"issue_id":"pydantic-ai-gepa-51k.2.7","depends_on_id":"pydantic-ai-gepa-51k.2.4","type":"blocks","created_at":"2025-12-19T13:46:52.894772-08:00","created_by":"daemon","metadata":"{}"},{"issue_id":"pydantic-ai-gepa-51k.2.7","depends_on_id":"pydantic-ai-gepa-51k.2.6","type":"blocks","created_at":"2025-12-19T13:46:53.004856-08:00","created_by":"daemon","metadata":"{}"}]}
{"id":"pydantic-ai-gepa-51k.3","title":"GEPA: scalable skill component routing","description":"Make skill optimization scale to an unbounded number of skills by introducing trace-aware skill/component activation (search + activate), and ensure merge can recombine improvements across branches.","design":"# Epic plan template (RFC-like)\n\n## 0. Context snapshot\n- Repo/branch: `pydantic-ai-gepa` / `mh/skills`\n- Commit SHA: `210bc6f3d5ea8e9a39df5a1e7191c1a1c0535a5a`\n- Related issues/PRs:\n  - `pydantic-ai-gepa-51k` (Agent Skills: spec alignment + skills optimization)\n- Key commands used to inspect the code:\n  - `rg -n \"LlmComponentSelector|merge\" src/pydantic_ai_gepa/gepa_graph -S`\n  - `sed -n '1,220p' src/pydantic_ai_gepa/gepa_graph/selectors/component.py`\n  - `sed -n '1,260p' src/pydantic_ai_gepa/skill_components.py`\n  - `sed -n '220,340p' src/pydantic_ai_gepa/gepa_graph/proposal/instruction.py`\n  - `sed -n '1,240p' src/pydantic_ai_gepa/gepa_graph/proposal/merge.py`\n\n## 1. Summary\nWe need a component-selection + optimization workflow that scales to an effectively unbounded number of skills. The current approach models every skill file as a GEPA component up front, which will not scale. The proposed approach is to introduce **trace-aware component activation**: the reflection system can search the skill pack, select a small set of relevant skills for the observed traces, and *activate* only those skill components into the candidate set (with baseline values). Subsequent reflections then optimize only the activated components, and merge can combine improvements across different branches.\n\n## 2. Goals / Non-goals\n\n### Goals\n- Allow a run to start with **zero or few** skill components and expand only when the traces suggest a skill is relevant.\n- Provide a reflection-time mechanism to **search skills** and select which ones to optimize (without enumerating all skills in the prompt).\n- Preserve the ability to **accumulate optimizations** across iterations and datasets by carrying forward only the modified skills.\n- Keep **merge/crossover** meaningful: it should be able to combine independent improvements (e.g. skill A improved in one branch, skill B in another).\n\n### Non-goals\n- Arbitrary execution of skill scripts as part of optimization (keep skills content read-only aside from text updates).\n- Perfect global optimality; we want pragmatic scalability and robust tooling.\n\n## 3. Existing code analysis\n\n### Related patterns\n- Skills as components: `src/pydantic_ai_gepa/skill_components.py`\n  - `extract_skill_components()` currently extracts *all* skills (description/body + `examples/`) into the candidate map.\n  - `apply_candidate_to_skills()` applies only the candidate’s skill keys via an overlay FS (good for sparse updates).\n- Component selection: `src/pydantic_ai_gepa/gepa_graph/selectors/component.py`\n  - `AllComponentSelector` and `RoundRobinComponentSelector` scale poorly when component count is huge.\n  - `LlmComponentSelector` bounds prompt size (`max_allowed_components`) but still requires a candidate containing all skills.\n- Proposal generation requires components to exist: `src/pydantic_ai_gepa/gepa_graph/proposal/instruction.py`\n  - `InstructionProposalGenerator.propose_texts()` raises if a selected component is missing from the candidate.\n- Merge currently assumes identical component sets: `src/pydantic_ai_gepa/gepa_graph/proposal/merge.py`\n  - `build_merged_candidate()` raises if parents don’t share identical component key sets.\n\n### Current behavior (what happens today)\n- When `skills=...` is passed to `optimize_agent`, the seed candidate is augmented with **every** skill’s description/body and every `examples/**` file.\n- Component selection is performed against the full candidate key set.\n- Accepted candidates schedule merge attempts (`ReflectStep` -\u003e `ContinueStep` -\u003e `MergeStep`).\n\n### Key invariants / constraints\n- Reflection proposer assumes selected components are present in the candidate.\n- Merge assumes a consistent “component universe” between parents.\n- Skills application already supports *sparse* overlays (candidate only needs to include modified skill keys).\n\n## 4. Technical design\n\n### Data model / state changes\n- Add an **active components registry** to `GepaState` for skills (and optionally other large component families):\n  - `active_skill_paths: set[str]` (and maybe `active_skill_files: set[tuple[str, str]]` for examples)\n  - This registry drives which skill component keys are materialized into candidates.\n\n### API / integration changes\n- Add a reflection-time tool interface (used by a selector/proposer) to:\n  1) `search_skills(query, top_k)` using the existing `SkillsSearchProvider` over the *base* skill pack (plus current candidate overlay)\n  2) `activate_skill(skill_path)` to materialize that skill’s components into the run’s candidate universe\n  3) Optional: `activate_skill_file(skill_path, path)` for example files\n\n### Component materialization\n- Activation should materialize baseline components by reading from the current skills FS (base + overlay) and adding:\n  - `skill:{skill_path}:frontmatter:description`\n  - `skill:{skill_path}:body`\n  - Optionally `skill:{skill_path}:file:examples/...`\n- Once a skill is activated, its component keys are added with baseline values to:\n  - The current parent candidate\n  - The run’s `seed_candidate`\n  - All existing candidates in state (filled with baseline) so merge invariants hold\n\n### Selection behavior (LLM at the right level)\n- Instead of passing *all* skills to `LlmComponentSelector`, the selector gets:\n  - Core agent component keys (instructions/tools/etc.)\n  - A bounded list of currently activated skill keys\n  - Tools to search and activate additional skills when traces suggest missing knowledge\n- The selector should be trace-aware:\n  - Parse tool calls/returns from trajectories (e.g. which skill paths were loaded)\n  - Prefer optimizing skills that were consulted but still produced failures, or skills that should have been consulted but weren’t\n\n### How merge builds upon previous optimizations\n- Reflection creates branches: different accepted candidates can improve different subsets of activated components.\n- Merge attempts to recombine these independent improvements using a 3-way “ancestor vs parent1 vs parent2” rule.\n- With activation + baseline backfilling, parents keep identical component sets, enabling the existing merge crossover algorithm to work without needing a union/implicit-base merge rewrite.\n\n### Key technical decisions\n- **Decision: activation + backfill rather than optimizing all skills up front**\n  - Why: candidate size scales with *optimized* skills, not total skills.\n  - Alternatives considered:\n    - All skills as components + LLM selector prompt truncation (still huge in memory/state).\n    - Encode all skill edits in a single “skills_patch” component (harder to merge + less interpretable).\n  - Trade-offs: requires state mutation/backfill when activating skills; candidates grow over time.\n\n## 5. Implementation plan (sequenced, no timelines)\n\n### Proposed subtasks (bd child issues)\n1. Add `active_skill_paths` + activation API to `GepaState` (task, P1)\n   - Touch: `src/pydantic_ai_gepa/gepa_graph/models/state.py`, `src/pydantic_ai_gepa/gepa_graph/steps/*`\n2. Implement skill component materialization/backfill helper (task, P1)\n   - Touch: `src/pydantic_ai_gepa/skill_components.py`, `src/pydantic_ai_gepa/gepa_graph/steps/reflect.py`\n3. Add a trace-aware selector that can `search_skills` + `activate_skill` (feature, P1)\n   - Touch: `src/pydantic_ai_gepa/gepa_graph/selectors/component.py`, `src/pydantic_ai_gepa/gepa_graph/helpers.py`\n4. Add tests covering activation + merge invariants (task, P1)\n   - Touch: `tests/gepa_graph/selectors/*`, `tests/gepa_graph/steps/*`\n5. Update docs/examples to describe incremental skill optimization + merge (chore, P2)\n   - Touch: `README.md`, `docs/gepa.md`, `examples/optimize_skills.py`\n\n### Rollout \u0026 validation\n- Default behavior remains “extract all skills as components” initially; activation is opt-in behind a config flag (e.g. `skills_component_mode=\"activated\"`).\n- Validate with a medium-sized skill pack (hundreds of skills) and ensure memory and prompt sizes are stable.\n\n### Testing plan\n- Unit tests for activation/backfill (component keys added and baseline values correct).\n- Selector tests ensuring tool-call traces influence which skill components are preferred.\n- Merge tests where branch A improves skill X and branch B improves skill Y; merged candidate preserves both.\n\n## 6. Open questions\n- Should we activate examples (`examples/**`) by default, or only when explicitly requested by the selector?\n- How do we bound component growth over very long runs (eviction / freezing)?\n- Should skill activation happen in the selector (routing) or in the proposer (reflection agent)?\n\n## 7. Links / references\n- `docs/gepa.md` (merge + selection background)\n- `src/pydantic_ai_gepa/skill_components.py` (skills overlay mechanics)","acceptance_criteria":"- Candidate size scales with activated skills, not total skills\\n- Reflection can search + activate skills based on traces\\n- Merge can combine independent skill improvements\\n- Tests cover activation/backfill + merge","status":"open","priority":1,"issue_type":"epic","created_at":"2025-12-23T13:38:16.52201-08:00","updated_at":"2025-12-23T13:38:16.52201-08:00","dependencies":[{"issue_id":"pydantic-ai-gepa-51k.3","depends_on_id":"pydantic-ai-gepa-51k","type":"parent-child","created_at":"2025-12-23T13:38:16.523166-08:00","created_by":"daemon","metadata":"{}"}]}
{"id":"pydantic-ai-gepa-51k.3.1","title":"Skill activation registry in GepaState","description":"Add activated-skill tracking to state (no behavior change yet).","design":"# Subtask template (handoff-ready)\n\n## Objective\nIntroduce an activation registry in `GepaState` that tracks which skills/components are materialized into candidates, plus state helpers for scheduling/recording activations.\n\n## Context (existing code pointers)\n- Primary entry point(s): `src/pydantic_ai_gepa/gepa_graph/models/state.py` (config + runtime counters)\n- Related pattern(s) to follow: `src/pydantic_ai_gepa/gepa_graph/models/state.py` merge counters + methods (`schedule_merge`, `record_merge_attempt`)\n- Gotchas / constraints discovered during analysis:\n  - Proposal generation requires selected components exist in the candidate.\n  - Merge assumes parents share identical component sets.\n\n## Scope\n\n### In scope\n- Add fields to state to track activated skills/components.\n- Add small helper methods to activate skills/components and expose activated set.\n\n### Out of scope\n- Implementing the selector/proposer behavior that uses this registry.\n\n## Implementation notes\n- Files to change (expected):\n  - `src/pydantic_ai_gepa/gepa_graph/models/state.py`\n- Steps:\n  1. Add `active_skill_paths` (and optionally `active_skill_files`) to `GepaState`.\n  2. Add methods like `activate_skill_path(path)` and `is_skill_active(path)`.\n  3. Ensure defaults are empty and serialization/checkpointing remains compatible.\n\n## Deliverables\n- Code changes: state fields + helpers.\n- Tests: model/config tests verifying defaults.\n- Docs (only if required): none.\n\n## Acceptance criteria\n- State has explicit activated-skill tracking.\n- No behavior change unless activation is used.\n\n## Testing / verification\n- Commands to run: `uv run pytest -k state`.","acceptance_criteria":"- GepaState has activated-skill fields + helpers\\n- Defaults preserve existing behavior","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-23T13:39:04.103821-08:00","updated_at":"2025-12-23T13:39:04.103821-08:00","dependencies":[{"issue_id":"pydantic-ai-gepa-51k.3.1","depends_on_id":"pydantic-ai-gepa-51k.3","type":"parent-child","created_at":"2025-12-23T13:39:04.105174-08:00","created_by":"daemon","metadata":"{}"}]}
{"id":"pydantic-ai-gepa-51k.3.2","title":"Materialize + backfill skill components","description":"Implement baseline materialization for a skill and backfill into candidates/seed for merge compatibility.","design":"# Subtask template (handoff-ready)\n\n## Objective\nAdd a helper to materialize baseline skill components (description/body + optional example files) into candidates and backfill into all existing candidates to preserve merge invariants.\n\n## Context (existing code pointers)\n- Primary entry point(s):\n  - `src/pydantic_ai_gepa/skill_components.py` (`skill:*` key schema; overlay apply)\n  - `src/pydantic_ai_gepa/gepa_graph/proposal/merge.py` (requires identical key sets)\n- Related pattern(s) to follow:\n  - `src/pydantic_ai_gepa/gepa_graph/steps/evaluate.py:_hydrate_missing_components` (hydrates and updates seed candidate)\n- Gotchas / constraints discovered during analysis:\n  - Skill component keys are currently derived from filesystem layout.\n  - Backfilling should use baseline values from skills FS (base + current overlay).\n\n## Scope\n\n### In scope\n- Implement a function that, given `SkillsFS|OverlayFS` and `skill_path`, returns `CandidateMap` entries for that skill (desc/body + optional files).\n- Implement a backfill helper that adds these components to all candidates + deps.seed_candidate.\n\n### Out of scope\n- Building the LLM-based selector that decides *which* skills to activate.\n\n## Implementation notes\n- Files to change (expected):\n  - `src/pydantic_ai_gepa/skill_components.py`\n  - `src/pydantic_ai_gepa/gepa_graph/steps/reflect.py` (hook for activation/backfill)\n- Steps:\n  1. Add `load_skill_components_for_path(fs, skill_path, include_examples=...)`.\n  2. Add `backfill_components(state, deps, new_components)`.\n  3. Ensure backfill is deterministic and doesn’t overwrite already-modified values.\n\n## Deliverables\n- Code changes: materialization + backfill.\n- Tests: backfill preserves existing modifications.\n\n## Acceptance criteria\n- Activating a skill results in baseline components present in every candidate.\n- Merge no longer fails due to component-set mismatch after activation.\n\n## Testing / verification\n- Commands to run: `uv run pytest -k merge -k skills`.","acceptance_criteria":"- Activating a skill adds baseline components to all candidates\\n- Existing modifications are preserved\\n- Merge no longer fails from key mismatch after activation","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-23T13:39:04.291128-08:00","updated_at":"2025-12-23T13:39:04.291128-08:00","dependencies":[{"issue_id":"pydantic-ai-gepa-51k.3.2","depends_on_id":"pydantic-ai-gepa-51k.3","type":"parent-child","created_at":"2025-12-23T13:39:04.291972-08:00","created_by":"daemon","metadata":"{}"}]}
{"id":"pydantic-ai-gepa-51k.3.3","title":"Trace-aware selector with search+activate tools","description":"Add an LLM selector mode that searches skills based on traces and activates relevant skills before selecting components.","design":"# Subtask template (handoff-ready)\n\n## Objective\nImplement a trace-aware selector that can search the skills pack and activate relevant skills before choosing components to update.\n\n## Context (existing code pointers)\n- Primary entry point(s):\n  - `src/pydantic_ai_gepa/gepa_graph/selectors/component.py` (`LlmComponentSelector`)\n  - `src/pydantic_ai_gepa/gepa_graph/steps/reflect.py` (component selection + proposing)\n- Related pattern(s) to follow:\n  - `src/pydantic_ai_gepa/gepa_graph/proposal/student_tools.py` (skills search toolset)\n- Gotchas / constraints discovered during analysis:\n  - Proposal generator requires component keys exist.\n  - Selector should not enumerate all skills; it should use a search tool.\n\n## Scope\n\n### In scope\n- Add tool(s) usable by the selector/reflection agent:\n  - `search_skills(query, top_k)` returning `SkillSearchResult`.\n  - `activate_skill(skill_path)` which backfills baseline components into state.\n- Add a selector mode (new `component_selector` literal or config flag) that uses these tools.\n\n### Out of scope\n- External indexing changes (turbopuffer) beyond reusing existing `SkillsSearchProvider`.\n\n## Implementation notes\n- Files to change (expected):\n  - `src/pydantic_ai_gepa/gepa_graph/selectors/component.py`\n  - `src/pydantic_ai_gepa/gepa_graph/helpers.py`\n  - `src/pydantic_ai_gepa/gepa_graph/models/state.py`\n- Steps:\n  1. Add a new selector implementation (e.g. `ActivatedSkillsLlmComponentSelector`).\n  2. Provide it with a toolset backed by the skills FS/search provider.\n  3. Update `create_deps` to wire in required dependencies.\n\n## Deliverables\n- Code changes: selector + wiring.\n- Tests: selector activates skill based on trace keywords and then selects its components.\n\n## Acceptance criteria\n- Selector can activate a skill (without preloading all skills) and then select skill component keys.\n\n## Testing / verification\n- Commands to run: `uv run pytest -k llm_component_selector -k skills`.","acceptance_criteria":"- Selector can search skills + activate\\n- Selector chooses only activated skill components\\n- Tests cover activation-driven selection","status":"open","priority":1,"issue_type":"feature","created_at":"2025-12-23T13:39:04.488566-08:00","updated_at":"2025-12-23T13:39:04.488566-08:00","dependencies":[{"issue_id":"pydantic-ai-gepa-51k.3.3","depends_on_id":"pydantic-ai-gepa-51k.3","type":"parent-child","created_at":"2025-12-23T13:39:04.489232-08:00","created_by":"daemon","metadata":"{}"}]}
{"id":"pydantic-ai-gepa-51k.3.4","title":"Docs: incremental skill optimization + merge","description":"Document scalable skills selection/activation and how merge accumulates improvements; update optimize_skills example.","design":"# Subtask template (handoff-ready)\n\n## Objective\nDocument incremental skill optimization and how merge helps recombine improvements; update the skills optimization example accordingly.\n\n## Context (existing code pointers)\n- Primary entry point(s): `README.md`, `docs/gepa.md`, `examples/optimize_skills.py`\n- Related pattern(s) to follow: existing docs in `docs/gepa.md` for merge background\n\n## Scope\n\n### In scope\n- Explain:\n  - Why “all skills as components” doesn’t scale.\n  - How activation works and how to carry forward skill improvements across runs.\n  - How merge recombines improvements.\n- Update `examples/optimize_skills.py` to show the recommended selector/config.\n\n### Out of scope\n- Rewriting the whole GEPA spec.\n\n## Implementation notes\n- Files to change (expected):\n  - `README.md`\n  - `docs/gepa.md`\n  - `examples/optimize_skills.py`\n- Steps:\n  1. Add a short section describing merge + selector modes.\n  2. Add a section describing skill activation mode.\n\n## Deliverables\n- Docs + example update.\n\n## Acceptance criteria\n- Docs clarify merge and the scalable selection story.\n\n## Testing / verification\n- Commands to run: `uv run python examples/optimize_skills.py` (optional).","acceptance_criteria":"- README/docs mention merge + activation\\n- Example updated","status":"open","priority":2,"issue_type":"chore","created_at":"2025-12-23T13:39:04.680894-08:00","updated_at":"2025-12-23T13:39:04.680894-08:00","dependencies":[{"issue_id":"pydantic-ai-gepa-51k.3.4","depends_on_id":"pydantic-ai-gepa-51k.3","type":"parent-child","created_at":"2025-12-23T13:39:04.682121-08:00","created_by":"daemon","metadata":"{}"}]}
